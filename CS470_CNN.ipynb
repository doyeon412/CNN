{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS470_CNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vajOqm1DPhoQ","colab_type":"code","outputId":"3a5becb3-a9ca-4869-beb4-5f9c36b70966","executionInfo":{"status":"ok","timestamp":1574485933789,"user_tz":-540,"elapsed":878,"user":{"displayName":"김도연","photoUrl":"","userId":"06546154812880457616"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["from google.colab import drive\n","\n","drive.mount('/gdrive')\n","gdrive_root = '/gdrive/My Drive'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"78mGWCdYPm9B","colab_type":"code","outputId":"283fd5a3-ecc5-42bd-cc5a-79d6ad598bce","executionInfo":{"status":"ok","timestamp":1574485938738,"user_tz":-540,"elapsed":998,"user":{"displayName":"김도연","photoUrl":"","userId":"06546154812880457616"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import Adam, lr_scheduler\n","from torchvision import transforms, datasets\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import math\n","from PIL import Image\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"device setting: {device}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["device setting: cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_LBbjVQ_Q6Y1","colab_type":"code","colab":{}},"source":["# training & optimization hyper-parameters\n","max_epoch = 20\n","learning_rate = 0.001\n","\n","# model hyper-parameters\n","output_dim = 10 \n","\n","# Boolean value to select training process\n","training_process = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TRQ1XryoPqDE","colab_type":"code","colab":{}},"source":["def load_mnist(path='/gdrive/my_data', batch_size=100, shift_pixels=2):\n","    \"\"\"\n","    Construct dataloaders for training and test data. Data augmentation is also done here.\n","    :param path: file path of the dataset\n","    :param download: whether to download the original data\n","    :param batch_size: batch size\n","    :param shift_pixels: maximum number of pixels to shift in each direction\n","    :return: train_loader, test_loader\n","    \"\"\"\n","    kwargs = {'num_workers': 1, 'pin_memory': True} \n","    try:\n","      train_loader = torch.utils.data.DataLoader(\n","          datasets.MNIST(path, train=True, transform=transforms.Compose([transforms.RandomCrop(size=28, padding=shift_pixels), transforms.ToTensor()])), batch_size=batch_size, shuffle=True, **kwargs)\n","      test_loader = torch.utils.data.DataLoader(\n","          datasets.MNIST(path, train=False, transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True, **kwargs)\n","    except:\n","      train_loader = torch.utils.data.DataLoader(\n","          datasets.MNIST(path, train=True,download=True, transform=transforms.Compose([transforms.RandomCrop(size=28, padding=shift_pixels), transforms.ToTensor()])), batch_size=batch_size, shuffle=True, **kwargs)\n","      test_loader = torch.utils.data.DataLoader(\n","          datasets.MNIST(path, train=False,download=True, transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True, **kwargs)\n","\n","    return train_loader, test_loader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jMPdXtiPt-Q","colab_type":"code","colab":{}},"source":["class MyClassifier(nn.Module):\n","  def __init__(self):\n","        super(MyClassifier, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=5)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5)\n","        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=5)\n","        self.fc1 = nn.Linear(in_features=256 * 4 * 4, out_features=328)\n","        self.fc2 = nn.Linear(in_features=328, out_features=192)\n","        self.fc3 = nn.Linear(in_features=192, out_features=output_dim)\n","\n","  def forward(self, x):\n","      x = self.pool(self.relu((self.conv1(x))))\n","      x = self.relu(self.conv2(x))\n","      x = self.relu((self.conv3(x)))\n","      x = x.view(-1, 256 * 4 * 4)\n","      x = self.relu(self.fc1(x))\n","      x = self.relu(self.fc2(x))\n","      outputs = self.fc3(x)\n","      return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UaJzSd9FQPy3","colab_type":"code","outputId":"4dd88306-da78-4941-9824-f96eabf24180","executionInfo":{"status":"ok","timestamp":1574486905910,"user_tz":-540,"elapsed":1419,"user":{"displayName":"김도연","photoUrl":"","userId":"06546154812880457616"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["my_classifier = MyClassifier()\n","my_classifier = my_classifier.to(device)\n","\n","# Print your neural network structure\n","print(my_classifier)\n","\n","optimizer = optim.Adam(my_classifier.parameters(), lr=learning_rate)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["MyClassifier(\n","  (conv1): Conv2d(1, 128, kernel_size=(5, 5), stride=(1, 1))\n","  (relu): ReLU()\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n","  (conv3): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=4096, out_features=328, bias=True)\n","  (fc2): Linear(in_features=328, out_features=192, bias=True)\n","  (fc3): Linear(in_features=192, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u3UfMvo2Qjwk","colab_type":"code","colab":{}},"source":["ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n","if not os.path.exists(ckpt_dir):\n","  os.makedirs(ckpt_dir)\n","  \n","best_acc = 0.\n","ckpt_path = os.path.join(ckpt_dir, 'lastest.pt')\n","if os.path.exists(ckpt_path):\n","  ckpt = torch.load(ckpt_path)\n","  try:\n","    my_classifier.load_state_dict(ckpt['my_classifier'])\n","    optimizer.load_state_dict(ckpt['optimizer'])\n","    best_acc = ckpt['best_acc']\n","  except RuntimeError as e:\n","      print('wrong checkpoint')\n","  else:    \n","    print('checkpoint is loaded !')\n","    print('current best accuracy : %.2f' % best_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mdfWm0xQl6-","colab_type":"code","outputId":"45535998-6045-4081-a7f3-63e318eecc6d","colab":{"base_uri":"https://localhost:8080/","height":447}},"source":["if training_process:\n","  it = 0\n","  train_losses = []\n","  test_losses = []\n","  for epoch in range(max_epoch):\n","    # train phase\n","    my_classifier.train()\n","    \n","    train_loader, test_loader = load_mnist(path=gdrive_root+'/my_data', batch_size=100)\n","    for inputs, labels in train_loader:\n","      it += 1\n","\n","      # load data to the GPU.\n","      inputs = inputs.to(device)\n","      labels = labels.to(device)\n","\n","      # feed data into the network and get outputs.\n","      logits = my_classifier(inputs)\n","\n","      # calculate loss\n","      # Note: `F.cross_entropy` function receives logits, or pre-softmax outputs, rather than final probability scores.\n","      loss = F.cross_entropy(logits, labels)\n","\n","      # Note: You should flush out gradients computed at the previous step before computing gradients at the current step. \n","      #       Otherwise, gradients will accumulate.\n","      optimizer.zero_grad()\n","\n","      # backprogate loss.\n","      loss.backward()\n","\n","      # update the weights in the network.\n","      optimizer.step()\n","\n","      # calculate accuracy.\n","      acc = (logits.argmax(dim=1) == labels).float().mean()\n","\n","      if it % 2000 == 0:\n","        print('[epoch:{}, iteration:{}] train loss : {:.4f} train accuracy : {:.4f}'.format(epoch, it, loss.item(), acc.item()))\n","\n","    # save losses in a list so that we can visualize them later.\n","    train_losses.append(loss)  \n","\n","    # test phase\n","    n = 0.\n","    test_loss = 0.\n","    test_acc = 0.\n","    my_classifier.eval()\n","    for test_inputs, test_labels in test_loader:\n","      test_inputs = test_inputs.to(device)\n","      test_labels = test_labels.to(device)\n","\n","      logits = my_classifier(test_inputs)\n","      test_loss += F.cross_entropy(logits, test_labels, reduction='sum').item()\n","      test_acc += (logits.argmax(dim=1) == test_labels).float().sum().item()\n","      n += test_inputs.size(0)\n","\n","    test_loss /= n\n","    test_acc /= n\n","    test_losses.append(test_loss)\n","    \n","    print('[epoch:{}, iteration:{}] test_loss : {:.4f} test accuracy : {:.4f}'.format(epoch, it, test_loss, test_acc)) \n","\n","    # save checkpoint whenever there is improvement in performance\n","    if test_acc > best_acc:\n","      best_acc = test_acc\n","      # Note: optimizer also has states ! don't forget to save them as well.\n","      ckpt = {'my_classifier':my_classifier.state_dict(),\n","              'optimizer':optimizer.state_dict(),\n","              'best_acc':best_acc}\n","      torch.save(ckpt, ckpt_path)\n","      print('checkpoint is saved !')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[epoch:0, iteration:600] test_loss : 0.0319 test accuracy : 0.9908\n","checkpoint is saved !\n","[epoch:1, iteration:1200] test_loss : 0.0220 test accuracy : 0.9931\n","checkpoint is saved !\n","[epoch:2, iteration:1800] test_loss : 0.0231 test accuracy : 0.9925\n","[epoch:3, iteration:2000] train loss : 0.0056 train accuracy : 1.0000\n","[epoch:3, iteration:2400] test_loss : 0.0300 test accuracy : 0.9925\n","[epoch:4, iteration:3000] test_loss : 0.0256 test accuracy : 0.9923\n","[epoch:5, iteration:3600] test_loss : 0.0245 test accuracy : 0.9930\n","[epoch:6, iteration:4000] train loss : 0.0415 train accuracy : 0.9900\n","[epoch:6, iteration:4200] test_loss : 0.0245 test accuracy : 0.9930\n","[epoch:7, iteration:4800] test_loss : 0.0255 test accuracy : 0.9936\n","checkpoint is saved !\n","[epoch:8, iteration:5400] test_loss : 0.0273 test accuracy : 0.9919\n","[epoch:9, iteration:6000] train loss : 0.0029 train accuracy : 1.0000\n","[epoch:9, iteration:6000] test_loss : 0.0221 test accuracy : 0.9934\n","[epoch:10, iteration:6600] test_loss : 0.0199 test accuracy : 0.9948\n","checkpoint is saved !\n","[epoch:11, iteration:7200] test_loss : 0.0269 test accuracy : 0.9937\n","[epoch:12, iteration:7800] test_loss : 0.0169 test accuracy : 0.9951\n","checkpoint is saved !\n","[epoch:13, iteration:8000] train loss : 0.0015 train accuracy : 1.0000\n","[epoch:13, iteration:8400] test_loss : 0.0308 test accuracy : 0.9933\n"],"name":"stdout"}]}]}